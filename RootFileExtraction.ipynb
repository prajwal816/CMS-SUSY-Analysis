{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14871053,"sourceType":"datasetVersion","datasetId":9513318},{"sourceId":14871289,"sourceType":"datasetVersion","datasetId":9513381},{"sourceId":14871351,"sourceType":"datasetVersion","datasetId":9513421},{"sourceId":14871442,"sourceType":"datasetVersion","datasetId":9513364}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install \"uproot>=5\" awkward vector rich tqdm pandas pyarrow fastparquet matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T05:26:12.553043Z","iopub.execute_input":"2026-02-18T05:26:12.553561Z","iopub.status.idle":"2026-02-18T05:26:20.899849Z","shell.execute_reply.started":"2026-02-18T05:26:12.553528Z","shell.execute_reply":"2026-02-18T05:26:20.899150Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.8/393.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.7/656.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# If needed:\n# !pip -q install uproot awkward numpy\n\nimport os, glob, json\nfrom pathlib import Path\n\nimport numpy as np\nimport uproot\nimport awkward as ak\n\n# ----------------------------\n# Inputs (your dataset paths)\n# ----------------------------\nDATASET_PATHS = {\n    \"SMS-TChiWZ_ZToLL\": \"/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll\",\n    \"DYJetsToLL_0J_TuneCP5\": \"/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5\",\n    \"WJetsToLNu_TuneCP5\": \"/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5\",\n    \"TTJets_TuneCP5\": \"/kaggle/input/datasets/darkangel411/ttjets-tunecp5\",\n}\n\nOUT_BASE = \"/kaggle/working/derivedroot\"\nos.makedirs(OUT_BASE, exist_ok=True)\n\nSTEP_SIZE = 100_000     # entries per chunk (tune if needed)\nMAX_EVENTS = None       # set an int for quick testing\n\nJET_PT_MIN = 30.0\n\n# -----------------------------------------\n# Output feature names (exactly your 50)\n# -----------------------------------------\nFEATURES50 = [\n    \"nMuon\",\"nElectron\",\"nJet\",\"MET_pt\",\"MET_phi\",\"MET_sumEt\",\n    \"Muon_pt_0\",\"Muon_eta_0\",\"Muon_phi_0\",\n    \"Muon_pt_1\",\"Muon_eta_1\",\"Muon_phi_1\",\n    \"Electron_pt_0\",\"Electron_eta_0\",\"Electron_phi_0\",\n    \"Electron_pt_1\",\"Electron_eta_1\",\"Electron_phi_1\",\n    \"Jet_pt_0\",\"Jet_eta_0\",\"Jet_phi_0\",\n    \"Jet_pt_1\",\"Jet_eta_1\",\"Jet_phi_1\",\n    \"Jet_pt_2\",\"Jet_eta_2\",\"Jet_phi_2\",\n    \"Jet_pt_3\",\"Jet_eta_3\",\"Jet_phi_3\",\n    \"HT\",\"ST\",\n    \"M_ll\",\"M_jj_01\",\"M_jj_12\",\n    \"delta_phi_MET_j0\",\"delta_phi_MET_j1\",\"min_delta_phi_MET_jets\",\n    \"delta_R_j0_j1\",\"delta_phi_ll\",\"delta_R_ll\",\n    \"Jet_btagDeepB_0\",\"Jet_btagDeepB_1\",\n    \"MT_lep_MET\",\"HT_ratio\",\"MET_pt_HT_ratio\",\n    \"nJet_pt30\",\"Jet_mass_0\",\"LeadLepton_pt\",\"sum_pt_leptons\"\n]\n\n# -----------------------------------------\n# Helpers: find ROOT files via file_index.json\n# -----------------------------------------\ndef _find_root_strings(obj):\n    if isinstance(obj, str):\n        if obj.lower().endswith(\".root\"):\n            yield obj\n    elif isinstance(obj, dict):\n        for v in obj.values():\n            yield from _find_root_strings(v)\n    elif isinstance(obj, (list, tuple)):\n        for it in obj:\n            yield from _find_root_strings(it)\n\ndef collect_root_files(dataset_dir):\n    dataset_dir = str(dataset_dir)\n    json_paths = glob.glob(os.path.join(dataset_dir, \"**\", \"*file_index.json*\"), recursive=True)\n    roots = set()\n\n    for jp in json_paths:\n        try:\n            with open(jp, \"r\") as f:\n                data = json.load(f)\n            for s in _find_root_strings(data):\n                roots.add(s)\n        except Exception as e:\n            print(f\"[WARN] Could not parse {jp}: {e}\")\n\n    # fallback: scan for .root directly if no index was found\n    if not roots:\n        for rp in glob.glob(os.path.join(dataset_dir, \"**\", \"*.root\"), recursive=True):\n            roots.add(rp)\n\n    return sorted(roots), sorted(json_paths)\n\n# -----------------------------------------\n# Physics/math helpers\n# -----------------------------------------\ndef delta_phi(phi1, phi2):\n    d = phi1 - phi2\n    return ak.abs((d + np.pi) % (2*np.pi) - np.pi)\n\ndef delta_r(eta1, phi1, eta2, phi2):\n    dphi = delta_phi(phi1, phi2)\n    return ak.sqrt((eta1 - eta2)**2 + dphi**2)\n\ndef pad_take(jagged, i, fill=0.0):\n    # jagged: ak.Array of per-event variable-length arrays\n    padded = ak.pad_none(jagged, i+1, clip=True)\n    return ak.fill_none(padded[:, i], fill)\n\ndef vec_mass(pt1, eta1, phi1, m1, pt2, eta2, phi2, m2):\n    # Build 4-vectors from (pt, eta, phi, m)\n    px1 = pt1 * np.cos(phi1); py1 = pt1 * np.sin(phi1); pz1 = pt1 * np.sinh(eta1)\n    px2 = pt2 * np.cos(phi2); py2 = pt2 * np.sin(phi2); pz2 = pt2 * np.sinh(eta2)\n    e1 = ak.sqrt(px1*px1 + py1*py1 + pz1*pz1 + m1*m1)\n    e2 = ak.sqrt(px2*px2 + py2*py2 + pz2*pz2 + m2*m2)\n    px = px1 + px2; py = py1 + py2; pz = pz1 + pz2; e = e1 + e2\n    m2tot = e*e - (px*px + py*py + pz*pz)\n    return ak.sqrt(ak.where(m2tot > 0, m2tot, 0))\n\ndef safe_div(num, den):\n    return ak.where(den != 0, num/den, 0)\n\n# -----------------------------------------\n# Determine tree name + pick btag branch\n# -----------------------------------------\nTREE_PREFERRED = [\"Events\", \"tree\", \"ntuple\", \"T\"]\n\ndef pick_tree_name(fin):\n    keys = [k.split(\";\")[0] for k in fin.keys()]\n    for cand in TREE_PREFERRED:\n        if cand in keys:\n            return cand\n    # fallback: first TTree\n    for k in keys:\n        try:\n            obj = fin[k]\n            if isinstance(obj, uproot.behaviors.TTree.TTree):\n                return k\n        except Exception:\n            pass\n    raise RuntimeError(f\"No TTree found. Keys sample: {keys[:30]}\")\n\ndef choose_btag_branch(branches_set):\n    # Prefer DeepCSV-style name if present; fallback options\n    for cand in [\"Jet_btagDeepB\", \"Jet_btagDeepFlavB\", \"Jet_btagCSVV2\"]:\n        if cand in branches_set:\n            return cand\n    return None\n\n# -----------------------------------------\n# Compute 50 derived features for one chunk\n# chunk is dict-like mapping branch -> ak.Array\n# -----------------------------------------\ndef compute_features(chunk, btag_branch=None):\n    # Raw branches (NanoAOD-style)\n    Mu_pt  = chunk.get(\"Muon_pt\",  ak.Array([[]]*len(next(iter(chunk.values())))))\n    Mu_eta = chunk.get(\"Muon_eta\", ak.Array([[]]*len(next(iter(chunk.values())))))\n    Mu_phi = chunk.get(\"Muon_phi\", ak.Array([[]]*len(next(iter(chunk.values())))))\n\n    El_pt  = chunk.get(\"Electron_pt\",  ak.Array([[]]*len(next(iter(chunk.values())))))\n    El_eta = chunk.get(\"Electron_eta\", ak.Array([[]]*len(next(iter(chunk.values())))))\n    El_phi = chunk.get(\"Electron_phi\", ak.Array([[]]*len(next(iter(chunk.values())))))\n\n    J_pt   = chunk.get(\"Jet_pt\",   ak.Array([[]]*len(next(iter(chunk.values())))))\n    J_eta  = chunk.get(\"Jet_eta\",  ak.Array([[]]*len(next(iter(chunk.values())))))\n    J_phi  = chunk.get(\"Jet_phi\",  ak.Array([[]]*len(next(iter(chunk.values())))))\n    J_mass = chunk.get(\"Jet_mass\", ak.zeros_like(pad_take(J_pt, 0, 0.0)))  # scalar default; overwritten below if present\n    if \"Jet_mass\" in chunk:\n        J_mass = chunk[\"Jet_mass\"]\n\n    MET_pt   = chunk.get(\"MET_pt\",   ak.zeros_like(pad_take(J_pt, 0, 0.0)))\n    MET_phi  = chunk.get(\"MET_phi\",  ak.zeros_like(MET_pt))\n    MET_sumE = chunk.get(\"MET_sumEt\", ak.zeros_like(MET_pt))\n\n    # Counts\n    nMuon = ak.num(Mu_pt, axis=1)\n    nElectron = ak.num(El_pt, axis=1)\n    nJet = ak.num(J_pt, axis=1)\n\n    # Leading/subleading objects\n    out = {}\n    out[\"nMuon\"] = ak.to_numpy(nMuon, allow_missing=False).astype(np.int32)\n    out[\"nElectron\"] = ak.to_numpy(nElectron, allow_missing=False).astype(np.int32)\n    out[\"nJet\"] = ak.to_numpy(nJet, allow_missing=False).astype(np.int32)\n\n    out[\"MET_pt\"] = ak.to_numpy(MET_pt).astype(np.float32)\n    out[\"MET_phi\"] = ak.to_numpy(MET_phi).astype(np.float32)\n    out[\"MET_sumEt\"] = ak.to_numpy(MET_sumE).astype(np.float32)\n\n    # Muons 0/1\n    out[\"Muon_pt_0\"]  = ak.to_numpy(pad_take(Mu_pt, 0, 0.0)).astype(np.float32)\n    out[\"Muon_eta_0\"] = ak.to_numpy(pad_take(Mu_eta,0, 0.0)).astype(np.float32)\n    out[\"Muon_phi_0\"] = ak.to_numpy(pad_take(Mu_phi,0, 0.0)).astype(np.float32)\n\n    out[\"Muon_pt_1\"]  = ak.to_numpy(pad_take(Mu_pt, 1, 0.0)).astype(np.float32)\n    out[\"Muon_eta_1\"] = ak.to_numpy(pad_take(Mu_eta,1, 0.0)).astype(np.float32)\n    out[\"Muon_phi_1\"] = ak.to_numpy(pad_take(Mu_phi,1, 0.0)).astype(np.float32)\n\n    # Electrons 0/1\n    out[\"Electron_pt_0\"]  = ak.to_numpy(pad_take(El_pt, 0, 0.0)).astype(np.float32)\n    out[\"Electron_eta_0\"] = ak.to_numpy(pad_take(El_eta,0, 0.0)).astype(np.float32)\n    out[\"Electron_phi_0\"] = ak.to_numpy(pad_take(El_phi,0, 0.0)).astype(np.float32)\n\n    out[\"Electron_pt_1\"]  = ak.to_numpy(pad_take(El_pt, 1, 0.0)).astype(np.float32)\n    out[\"Electron_eta_1\"] = ak.to_numpy(pad_take(El_eta,1, 0.0)).astype(np.float32)\n    out[\"Electron_phi_1\"] = ak.to_numpy(pad_take(El_phi,1, 0.0)).astype(np.float32)\n\n    # Jets 0-3\n    for i in range(4):\n        out[f\"Jet_pt_{i}\"]  = ak.to_numpy(pad_take(J_pt, i, 0.0)).astype(np.float32)\n        out[f\"Jet_eta_{i}\"] = ak.to_numpy(pad_take(J_eta,i, 0.0)).astype(np.float32)\n        out[f\"Jet_phi_{i}\"] = ak.to_numpy(pad_take(J_phi,i, 0.0)).astype(np.float32)\n\n    out[\"Jet_mass_0\"] = ak.to_numpy(pad_take(J_mass, 0, 0.0)).astype(np.float32)\n\n    # HT, nJet_pt30\n    jet_pt30 = J_pt[J_pt > JET_PT_MIN]\n    HT = ak.sum(jet_pt30, axis=1)\n    nJet_pt30 = ak.sum(J_pt > JET_PT_MIN, axis=1)\n    out[\"HT\"] = ak.to_numpy(HT).astype(np.float32)\n    out[\"nJet_pt30\"] = ak.to_numpy(nJet_pt30).astype(np.int32)\n\n    # Build combined leptons for M_ll, delta_phi_ll, delta_R_ll, sum_pt_leptons, MT_lep_MET\n    MU_MASS = 0.105658\n    EL_MASS = 0.000511\n\n    mu = ak.zip({\"pt\": Mu_pt, \"eta\": Mu_eta, \"phi\": Mu_phi, \"m\": ak.zeros_like(Mu_pt) + MU_MASS})\n    el = ak.zip({\"pt\": El_pt, \"eta\": El_eta, \"phi\": El_phi, \"m\": ak.zeros_like(El_pt) + EL_MASS})\n    lep = ak.concatenate([mu, el], axis=1)\n    lep_sorted = lep[ak.argsort(lep.pt, axis=1, ascending=False)]\n\n    lep0 = ak.pad_none(lep_sorted, 1, clip=True)[:, 0]\n    lep1 = ak.pad_none(lep_sorted, 2, clip=True)[:, 1]\n\n    lep0_pt  = ak.fill_none(lep0.pt, 0.0)\n    lep0_eta = ak.fill_none(lep0.eta, 0.0)\n    lep0_phi = ak.fill_none(lep0.phi, 0.0)\n\n    lep1_pt  = ak.fill_none(lep1.pt, 0.0)\n    lep1_eta = ak.fill_none(lep1.eta, 0.0)\n    lep1_phi = ak.fill_none(lep1.phi, 0.0)\n\n    # LeadLepton_pt = max(leading mu pT, leading e pT) with 0 if none\n    lead_mu_pt = pad_take(Mu_pt, 0, 0.0)\n    lead_el_pt = pad_take(El_pt, 0, 0.0)\n    LeadLepton_pt = ak.where(lead_mu_pt > lead_el_pt, lead_mu_pt, lead_el_pt)\n    out[\"LeadLepton_pt\"] = ak.to_numpy(LeadLepton_pt).astype(np.float32)\n\n    sum_pt_leptons = lep0_pt + lep1_pt\n    out[\"sum_pt_leptons\"] = ak.to_numpy(sum_pt_leptons).astype(np.float32)\n\n    # ST = HT + MET + sum(leading lepton pT)  (here: first two combined leptons)\n    ST = HT + MET_pt + sum_pt_leptons\n    out[\"ST\"] = ak.to_numpy(ST).astype(np.float32)\n\n    # M_ll (0 if <2 leptons)\n    has2lep = ak.num(lep_sorted, axis=1) >= 2\n    mll = ak.where(\n        has2lep,\n        vec_mass(lep0_pt, lep0_eta, lep0_phi, ak.zeros_like(lep0_pt),\n                 lep1_pt, lep1_eta, lep1_phi, ak.zeros_like(lep1_pt)),\n        0.0\n    )\n    out[\"M_ll\"] = ak.to_numpy(mll).astype(np.float32)\n\n    # delta_phi_ll, delta_R_ll\n    dphi_ll = ak.where(has2lep, delta_phi(lep0_phi, lep1_phi), 0.0)\n    dR_ll = ak.where(has2lep, delta_r(lep0_eta, lep0_phi, lep1_eta, lep1_phi), 0.0)\n    out[\"delta_phi_ll\"] = ak.to_numpy(dphi_ll).astype(np.float32)\n    out[\"delta_R_ll\"] = ak.to_numpy(dR_ll).astype(np.float32)\n\n    # Dijet masses\n    j0_pt, j0_eta, j0_phi, j0_m = pad_take(J_pt,0,0.0), pad_take(J_eta,0,0.0), pad_take(J_phi,0,0.0), pad_take(J_mass,0,0.0)\n    j1_pt, j1_eta, j1_phi, j1_m = pad_take(J_pt,1,0.0), pad_take(J_eta,1,0.0), pad_take(J_phi,1,0.0), pad_take(J_mass,1,0.0)\n    j2_pt, j2_eta, j2_phi, j2_m = pad_take(J_pt,2,0.0), pad_take(J_eta,2,0.0), pad_take(J_phi,2,0.0), pad_take(J_mass,2,0.0)\n\n    has2j = nJet >= 2\n    has3j = nJet >= 3\n\n    mjj01 = ak.where(has2j, vec_mass(j0_pt,j0_eta,j0_phi,j0_m, j1_pt,j1_eta,j1_phi,j1_m), 0.0)\n    mjj12 = ak.where(has3j, vec_mass(j1_pt,j1_eta,j1_phi,j1_m, j2_pt,j2_eta,j2_phi,j2_m), 0.0)\n    out[\"M_jj_01\"] = ak.to_numpy(mjj01).astype(np.float32)\n    out[\"M_jj_12\"] = ak.to_numpy(mjj12).astype(np.float32)\n\n    # Angular: MET vs jets\n    dphi_met_j0 = ak.where(has2j | (nJet>=1), delta_phi(MET_phi, pad_take(J_phi,0,0.0)), 0.0)\n    dphi_met_j1 = ak.where(has2j, delta_phi(MET_phi, pad_take(J_phi,1,0.0)), 0.0)\n\n    jets_phi_0to3 = ak.concatenate([ak.unzip(ak.zip({\"phi\": J_phi})).phi], axis=0)  # no-op; keep simple\n    jphi_pad = ak.pad_none(J_phi, 4, clip=True)\n    jphi0 = ak.fill_none(jphi_pad[:,0], 0.0)\n    jphi1 = ak.fill_none(jphi_pad[:,1], 0.0)\n    jphi2 = ak.fill_none(jphi_pad[:,2], 0.0)\n    jphi3 = ak.fill_none(jphi_pad[:,3], 0.0)\n    dphis = ak.stack([delta_phi(MET_phi, jphi0),\n                      delta_phi(MET_phi, jphi1),\n                      delta_phi(MET_phi, jphi2),\n                      delta_phi(MET_phi, jphi3)], axis=1)\n    # If fewer than 1 jet, define as 0\n    min_dphi = ak.where(nJet > 0, ak.min(dphis, axis=1), 0.0)\n\n    out[\"delta_phi_MET_j0\"] = ak.to_numpy(dphi_met_j0).astype(np.float32)\n    out[\"delta_phi_MET_j1\"] = ak.to_numpy(dphi_met_j1).astype(np.float32)\n    out[\"min_delta_phi_MET_jets\"] = ak.to_numpy(min_dphi).astype(np.float32)\n\n    # Angular: jets\n    dR_j0_j1 = ak.where(has2j, delta_r(pad_take(J_eta,0,0.0), pad_take(J_phi,0,0.0),\n                                      pad_take(J_eta,1,0.0), pad_take(J_phi,1,0.0)), 0.0)\n    out[\"delta_R_j0_j1\"] = ak.to_numpy(dR_j0_j1).astype(np.float32)\n\n    # B-tag leading/subleading\n    if btag_branch is not None and btag_branch in chunk:\n        b = chunk[btag_branch]\n        out[\"Jet_btagDeepB_0\"] = ak.to_numpy(pad_take(b, 0, 0.0)).astype(np.float32)\n        out[\"Jet_btagDeepB_1\"] = ak.to_numpy(pad_take(b, 1, 0.0)).astype(np.float32)\n    else:\n        out[\"Jet_btagDeepB_0\"] = ak.to_numpy(ak.zeros_like(MET_pt)).astype(np.float32)\n        out[\"Jet_btagDeepB_1\"] = ak.to_numpy(ak.zeros_like(MET_pt)).astype(np.float32)\n\n    # MT_lep_MET using leading combined lepton (lep0)\n    has1lep = ak.num(lep_sorted, axis=1) >= 1\n    dphi_lep_met = ak.where(has1lep, delta_phi(lep0_phi, MET_phi), 0.0)\n    MT = ak.where(has1lep, ak.sqrt(2*lep0_pt*MET_pt*(1 - ak.cos(dphi_lep_met))), 0.0)\n    out[\"MT_lep_MET\"] = ak.to_numpy(MT).astype(np.float32)\n\n    # Ratios\n    out[\"HT_ratio\"] = ak.to_numpy(safe_div(HT, HT + MET_pt)).astype(np.float32)\n    out[\"MET_pt_HT_ratio\"] = ak.to_numpy(safe_div(MET_pt, HT)).astype(np.float32)\n\n    # Sanity: enforce all 50 keys exist\n    for k in FEATURES50:\n        if k not in out:\n            raise RuntimeError(f\"Missing output feature '{k}' (bug in compute_features).\")\n\n    return out\n\n# -----------------------------------------\n# Main loop per dataset: stream -> write ROOT\n# -----------------------------------------\ndef write_derived_root(root_files, out_root_path, step_size=100_000, max_events=None):\n    if not root_files:\n        raise RuntimeError(\"No ROOT files found (from file_index.json or .root scan).\")\n\n    n_written = 0\n    wrote_tree = False\n\n    # Find a readable file to decide tree name + available branches\n    tree_name = None\n    branches = None\n    btag_branch = None\n\n    for rf in root_files:\n        try:\n            with uproot.open(rf) as fin:\n                tree_name = pick_tree_name(fin)\n                branches = set(fin[tree_name].keys())\n                btag_branch = choose_btag_branch(branches)\n                break\n        except Exception:\n            continue\n\n    if tree_name is None:\n        raise RuntimeError(\"Could not open any ROOT file to detect TTree/branches.\")\n\n    # Branches we want to read (intersection only; missing are handled as defaults)\n    needed = [\n        \"Muon_pt\",\"Muon_eta\",\"Muon_phi\",\n        \"Electron_pt\",\"Electron_eta\",\"Electron_phi\",\n        \"Jet_pt\",\"Jet_eta\",\"Jet_phi\",\"Jet_mass\",\n        \"MET_pt\",\"MET_phi\",\"MET_sumEt\",\n    ]\n    if btag_branch is not None:\n        needed.append(btag_branch)\n\n    expressions = [b for b in needed if b in branches]\n\n    with uproot.recreate(out_root_path) as fout:\n        for rf in root_files:\n            try:\n                with uproot.open(rf) as fin:\n                    if tree_name not in [k.split(\";\")[0] for k in fin.keys()]:\n                        continue\n                    tree = fin[tree_name]\n\n                    for chunk in tree.iterate(expressions, step_size=step_size, library=\"ak\"):\n                        feats = compute_features(chunk, btag_branch=btag_branch)\n\n                        if not wrote_tree:\n                            # Define output types from first chunk\n                            branch_types = {k: v.dtype for k, v in feats.items()}\n                            fout.mktree(\"Events\", branch_types)\n                            wrote_tree = True\n\n                        fout[\"Events\"].extend(feats)\n                        n_written += len(next(iter(feats.values())))\n\n                        if max_events is not None and n_written >= max_events:\n                            return n_written\n            except Exception as e:\n                print(f\"[WARN] Failed file {rf}: {e}\")\n\n    return n_written\n\n# Run all datasets\nfor ds_name, ds_path in DATASET_PATHS.items():\n    ds_out_dir = os.path.join(OUT_BASE, ds_name)\n    os.makedirs(ds_out_dir, exist_ok=True)\n\n    root_files, json_paths = collect_root_files(ds_path)\n    out_root = os.path.join(ds_out_dir, f\"derived_{ds_name}.root\")\n\n    manifest = {\n        \"dataset\": ds_name,\n        \"dataset_path\": ds_path,\n        \"out_root\": out_root,\n        \"root_files_found_count\": len(root_files),\n        \"root_files_found_preview\": root_files[:30],\n        \"file_index_json_found\": json_paths,\n        \"features_out\": FEATURES50,\n        \"step_size\": STEP_SIZE,\n        \"max_events\": MAX_EVENTS,\n    }\n    with open(os.path.join(ds_out_dir, \"manifest.json\"), \"w\") as f:\n        json.dump(manifest, f, indent=2)\n\n    print(f\"\\n=== {ds_name} ===\")\n    print(f\"[INFO] ROOT files found: {len(root_files)}\")\n    print(f\"[INFO] Writing: {out_root}\")\n\n    n = write_derived_root(root_files, out_root, step_size=STEP_SIZE, max_events=MAX_EVENTS)\n    print(f\"[DONE] events written: {n}\")\n    print(f\"[DONE] folder: {ds_out_dir}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T05:26:20.901428Z","iopub.execute_input":"2026-02-18T05:26:20.901679Z","iopub.status.idle":"2026-02-18T05:27:00.836543Z","shell.execute_reply.started":"2026-02-18T05:26:20.901652Z","shell.execute_reply":"2026-02-18T05:27:00.835594Z"}},"outputs":[{"name":"stdout","text":"[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_5: 'utf-8' codec can't decode byte 0x85 in position 16: invalid start byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_2: 'utf-8' codec can't decode byte 0xef in position 6: invalid continuation byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_0: 'utf-8' codec can't decode byte 0xed in position 17: invalid continuation byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_1: 'utf-8' codec can't decode byte 0xa1 in position 17: invalid start byte\n\n=== SMS-TChiWZ_ZToLL ===\n[INFO] ROOT files found: 0\n[INFO] Writing: /kaggle/working/derivedroot/SMS-TChiWZ_ZToLL/derived_SMS-TChiWZ_ZToLL.root\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2678490141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Writing: {out_root}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_derived_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EVENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DONE] events written: {n}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DONE] folder: {ds_out_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2678490141.py\u001b[0m in \u001b[0;36mwrite_derived_root\u001b[0;34m(root_files, out_root_path, step_size, max_events)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_derived_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mroot_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No ROOT files found (from file_index.json or .root scan).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mn_written\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No ROOT files found (from file_index.json or .root scan)."],"ename":"RuntimeError","evalue":"No ROOT files found (from file_index.json or .root scan).","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import os, glob\nfrom pathlib import Path\n\nprint(\"Top-level /kaggle/input entries:\")\nprint(os.listdir(\"/kaggle/input\"))\n\ndef quick_scan(p):\n    print(\"\\n---\")\n    print(\"Path:\", p, \"exists:\", os.path.exists(p))\n    if os.path.exists(p):\n        # show a few files\n        files = glob.glob(p + \"/**/*\", recursive=True)\n        print(\"Total entries under path:\", len(files))\n        print(\"Sample:\", files[:15])\n        roots = glob.glob(p + \"/**/*.root*\", recursive=True)\n        print(\"ROOT-like files found:\", len(roots))\n        print(\"ROOT sample:\", roots[:10])\n        idx = glob.glob(p + \"/**/*file_index.json*\", recursive=True)\n        print(\"file_index.json* found:\", len(idx))\n        print(\"index sample:\", idx[:10])\n\nDATASET_PATHS = {\n    \"SMS-TChiWZ_ZToLL\": \"/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll\",\n    \"DYJetsToLL_0J_TuneCP5\": \"/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5\",\n    \"WJetsToLNu_TuneCP5\": \"/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5\",\n    \"TTJets_TuneCP5\": \"/kaggle/input/datasets/darkangel411/ttjets-tunecp5\",\n}\n\nfor k,v in DATASET_PATHS.items():\n    print(f\"\\nDataset key: {k}\")\n    quick_scan(v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T05:29:45.483183Z","iopub.execute_input":"2026-02-18T05:29:45.483554Z","iopub.status.idle":"2026-02-18T05:29:45.519462Z","shell.execute_reply.started":"2026-02-18T05:29:45.483518Z","shell.execute_reply":"2026-02-18T05:29:45.518881Z"}},"outputs":[{"name":"stdout","text":"Top-level /kaggle/input entries:\n['datasets']\n\nDataset key: SMS-TChiWZ_ZToLL\n\n---\nPath: /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll exists: True\nTotal entries under path: 4\nSample: ['/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_5', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_2', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_0', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_1']\nROOT-like files found: 0\nROOT sample: []\nfile_index.json* found: 4\nindex sample: ['/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_5', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_2', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_0', '/kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_1']\n\nDataset key: DYJetsToLL_0J_TuneCP5\n\n---\nPath: /kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5 exists: True\nTotal entries under path: 4\nSample: ['/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_31', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_7', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_0', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_28']\nROOT-like files found: 0\nROOT sample: []\nfile_index.json* found: 4\nindex sample: ['/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_31', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_7', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_0', '/kaggle/input/datasets/g0ldeneagle/dyjetstoll-0j-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_40000_file_index.json_28']\n\nDataset key: WJetsToLNu_TuneCP5\n\n---\nPath: /kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5 exists: True\nTotal entries under path: 4\nSample: ['/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_9', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_10', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_7', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_0']\nROOT-like files found: 0\nROOT sample: []\nfile_index.json* found: 4\nindex sample: ['/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_9', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_10', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_7', '/kaggle/input/datasets/prajwalaaryan/wjetstolnu-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v2_270000_file_index.json_0']\n\nDataset key: TTJets_TuneCP5\n\n---\nPath: /kaggle/input/datasets/darkangel411/ttjets-tunecp5 exists: True\nTotal entries under path: 4\nSample: ['/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_51', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_1', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_10', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_0']\nROOT-like files found: 0\nROOT sample: []\nfile_index.json* found: 4\nindex sample: ['/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_51', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_1', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_10', '/kaggle/input/datasets/darkangel411/ttjets-tunecp5/CMS_mc_RunIISummer20UL16NanoAODv9_TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2530000_file_index.json_0']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os, glob, json, re\nfrom pathlib import Path\n\nROOT_RE = re.compile(r'([^\\s\"\\']+?\\.root)(\\?[^\\s\"\\']+)?', re.IGNORECASE)\n\ndef resolve_dataset_dir(p):\n    \"\"\"If user gave a non-existent path, try to find the dataset under /kaggle/input/<slug>.\"\"\"\n    if os.path.exists(p):\n        return p\n    slug = Path(p).name  # last token, often the dataset slug\n    cand = f\"/kaggle/input/{slug}\"\n    if os.path.exists(cand):\n        return cand\n    matches = glob.glob(f\"/kaggle/input/**/{slug}\", recursive=True)\n    if matches:\n        return matches[0]\n    return p  # fallback (will likely fail, but gives a clear debug)\n\ndef find_root_strings_anywhere(obj):\n    \"\"\"Find substrings containing '.root' even if they have querystrings or extra text.\"\"\"\n    if isinstance(obj, str):\n        m = ROOT_RE.search(obj)\n        if m:\n            yield m.group(1) + (m.group(2) or \"\")\n    elif isinstance(obj, dict):\n        for v in obj.values():\n            yield from find_root_strings_anywhere(v)\n    elif isinstance(obj, (list, tuple)):\n        for it in obj:\n            yield from find_root_strings_anywhere(it)\n\ndef collect_root_files(dataset_dir):\n    dataset_dir = resolve_dataset_dir(str(dataset_dir))\n\n    json_paths = glob.glob(os.path.join(dataset_dir, \"**\", \"*file_index.json*\"), recursive=True)\n    roots = set()\n\n    # 1) parse file_index.json* (json or json-lines)\n    for jp in json_paths:\n        try:\n            with open(jp, \"r\") as f:\n                txt = f.read()\n\n            # try normal JSON\n            try:\n                data = json.loads(txt)\n                for s in find_root_strings_anywhere(data):\n                    roots.add(s)\n            except Exception:\n                # try json-lines\n                for line in txt.splitlines():\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        data = json.loads(line)\n                        for s in find_root_strings_anywhere(data):\n                            roots.add(s)\n                    except Exception:\n                        # last resort: regex scan line\n                        m = ROOT_RE.search(line)\n                        if m:\n                            roots.add(m.group(1) + (m.group(2) or \"\"))\n\n        except Exception as e:\n            print(f\"[WARN] Could not parse {jp}: {e}\")\n\n    # 2) fallback: direct local scan for .root files\n    for rp in glob.glob(os.path.join(dataset_dir, \"**\", \"*.root*\"), recursive=True):\n        roots.add(rp)\n\n    roots = sorted(roots)\n    return roots, sorted(json_paths), dataset_dir\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T05:30:20.689742Z","iopub.execute_input":"2026-02-18T05:30:20.690054Z","iopub.status.idle":"2026-02-18T05:30:20.699765Z","shell.execute_reply.started":"2026-02-18T05:30:20.690027Z","shell.execute_reply":"2026-02-18T05:30:20.699190Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"for ds_name, ds_path in DATASET_PATHS.items():\n    root_files, json_paths, resolved = collect_root_files(ds_path)\n    print(f\"\\n=== {ds_name} ===\")\n    print(\"given:\", ds_path)\n    print(\"resolved:\", resolved)\n    print(\"file_index.json*:\", len(json_paths))\n    print(\"root files:\", len(root_files))\n    print(\"root preview:\", root_files[:5])\n\n    if not root_files:\n        raise RuntimeError(\n            f\"No ROOT files found for {ds_name}. \"\n            f\"Check the resolved path above and whether the dataset actually contains ROOT/tar/zip files.\"\n        )\n\n    # ... now call write_derived_root(root_files, ...)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T05:30:30.853703Z","iopub.execute_input":"2026-02-18T05:30:30.854429Z","iopub.status.idle":"2026-02-18T05:30:42.244985Z","shell.execute_reply.started":"2026-02-18T05:30:30.854400Z","shell.execute_reply":"2026-02-18T05:30:42.244067Z"}},"outputs":[{"name":"stdout","text":"[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_5: 'utf-8' codec can't decode byte 0x85 in position 16: invalid start byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_2: 'utf-8' codec can't decode byte 0xef in position 6: invalid continuation byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_0: 'utf-8' codec can't decode byte 0xed in position 17: invalid continuation byte\n[WARN] Could not parse /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll/CMS_mc_RunIISummer20UL16NanoAODv9_SMS-TChiWZ_ZToLL_mZMin-0p1_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_80000_file_index.json_1: 'utf-8' codec can't decode byte 0xa1 in position 17: invalid start byte\n\n=== SMS-TChiWZ_ZToLL ===\ngiven: /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll\nresolved: /kaggle/input/datasets/katakuricharlotte/sms-tchiwz-ztoll\nfile_index.json*: 4\nroot files: 0\nroot preview: []\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3754707106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mroot_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;34mf\"No ROOT files found for {ds_name}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;34mf\"Check the resolved path above and whether the dataset actually contains ROOT/tar/zip files.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No ROOT files found for SMS-TChiWZ_ZToLL. Check the resolved path above and whether the dataset actually contains ROOT/tar/zip files."],"ename":"RuntimeError","evalue":"No ROOT files found for SMS-TChiWZ_ZToLL. Check the resolved path above and whether the dataset actually contains ROOT/tar/zip files.","output_type":"error"}],"execution_count":8}]}